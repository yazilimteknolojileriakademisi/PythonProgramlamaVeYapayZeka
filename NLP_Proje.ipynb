{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8b50610",
   "metadata": {},
   "source": [
    "<H1>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MACHINE LEARNING PROJESİ</H1>\n",
    "\n",
    "## &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;NLP (Natural Language Processing) KULLANARAK IMDB FİLM \n",
    "\n",
    "## YORUMLARI KAGGLE DATA SETİ ÜZERİNDE SENTIMENT(DUYGU) ANALİZİ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f772e692",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52b4d4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veri setlerimizi yüklüyoruz..\n",
    "df = pd.read_csv('NLPlabeledData.tsv',  delimiter=\"\\t\", quoting=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d736f9b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"3630_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"It must be assumed that those who praised thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"9495_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review\n",
       "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
       "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
       "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
       "3  \"3630_4\"          0  \"It must be assumed that those who praised thi...\n",
       "4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verimize bakalım\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0be5437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "553cff4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[\"review\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e767dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sungu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stopwords'ü temizlemek için nltk kütüphanesinden stopwords kelime setini bilgisayarımıza indirmemiz gerekiyor. \n",
    "# Bu işlemi nltk ile yapıyoruz\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5748918b",
   "metadata": {},
   "source": [
    "## * * * * Veri Temizleme İşlemleri * * * *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bb420a",
   "metadata": {},
   "source": [
    "### Öncelikle BeautifulSoup modülünü kullanarak HTML taglerini review cümlelerinden sileceğiz.\n",
    "Bu işlemlerin nasıl yapıldığını açıklamak için önce örnek tek bir review seçip size nasıl yapıldığına bakalım:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52044274",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_review= df.review[0]\n",
    "sample_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d67c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML tagleri temizlendikten sonra..\n",
    "sample_review = BeautifulSoup(sample_review).get_text()\n",
    "sample_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a547704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# noktalama işaretleri ve sayılardan temizliyoruz - regex kullanarak..\n",
    "sample_review = re.sub(\"[^a-zA-Z]\",' ',sample_review)\n",
    "sample_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64917f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# küçük harfe dönüştürüyoruz, makine öğrenim algoritmalarımızın büyük harfle başlayan kelimeleri farklı kelime olarak\n",
    "# algılamaması için yapıyoruz bunu:\n",
    "sample_review = sample_review.lower()\n",
    "sample_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f3c5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords (yani the, is, are gibi kelimeler yapay zeka tarafından kullanılmamasını istiyoruz. Bunlar gramer kelimeri..)\n",
    "# önce split ile kelimeleri bölüyoruz ve listeye dönüştürüyoruz. amacımız stopwords kelimelerini çıkarmak..\n",
    "sample_review = sample_review.split()\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdd8373",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d729b143",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sample_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9a90aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_review without stopwords\n",
    "swords = set(stopwords.words(\"english\"))                      # conversion into set for fast searching\n",
    "sample_review = [w for w in sample_review if w not in swords]               \n",
    "sample_review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57027ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sample_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b90371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temizleme işlemini açıkladıktan sonra şimdi tüm dataframe'imiz içinde bulunan reviewleri döngü içinde topluca temizliyoruz\n",
    "# bu amaçla önce bir fonksiyon oluşturuyoruz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66ec33e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(review):\n",
    "    # review without HTML tags\n",
    "    review = BeautifulSoup(review).get_text()\n",
    "    # review without punctuation and numbers\n",
    "    review = re.sub(\"[^a-zA-Z]\",' ',review)\n",
    "    # converting into lowercase and splitting to eliminate stopwords\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    # review without stopwords\n",
    "    swords = set(stopwords.words(\"english\"))                      # conversion into set for fast searching\n",
    "    review = [w for w in review if w not in swords]               \n",
    "    # splitted paragraph'ları space ile birleştiriyoruz return\n",
    "    return(\" \".join(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "594912eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of reviews processed = 1000\n",
      "No of reviews processed = 2000\n",
      "No of reviews processed = 3000\n",
      "No of reviews processed = 4000\n",
      "No of reviews processed = 5000\n",
      "No of reviews processed = 6000\n",
      "No of reviews processed = 7000\n",
      "No of reviews processed = 8000\n",
      "No of reviews processed = 9000\n",
      "No of reviews processed = 10000\n",
      "No of reviews processed = 11000\n",
      "No of reviews processed = 12000\n",
      "No of reviews processed = 13000\n",
      "No of reviews processed = 14000\n",
      "No of reviews processed = 15000\n",
      "No of reviews processed = 16000\n",
      "No of reviews processed = 17000\n",
      "No of reviews processed = 18000\n",
      "No of reviews processed = 19000\n",
      "No of reviews processed = 20000\n",
      "No of reviews processed = 21000\n",
      "No of reviews processed = 22000\n",
      "No of reviews processed = 23000\n",
      "No of reviews processed = 24000\n",
      "No of reviews processed = 25000\n"
     ]
    }
   ],
   "source": [
    "# training datamızı yukardaki fonksiyon yardımıyla temizliyoruz: \n",
    "# her 1000 review sonrası bir satır yazdırarak review işleminin durumunu görüyoruz..\n",
    "\n",
    "train_x_tum = []\n",
    "for r in range(len(df[\"review\"])):        \n",
    "    if (r+1)%1000 == 0:        \n",
    "        print(\"No of reviews processed =\", r+1)\n",
    "    train_x_tum.append(process(df[\"review\"][r]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d2a1c0",
   "metadata": {},
   "source": [
    "### Train, test split..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d340b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_x_tum\n",
    "y = np.array(df[\"sentiment\"])\n",
    "\n",
    "# train test split\n",
    "train_x, test_x, y_train, y_test = train_test_split(x,y, test_size = 0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590370e4",
   "metadata": {},
   "source": [
    "### Bag of Words oluşturuyoruz !\n",
    "\n",
    "Verilerimizi temizledik ancak yapay zekanın çalışması için bu metin tabanlı verileri sayılara ve bag of words denilen matrise çevirmek gerekiyor. İşte bu amaçla sklearn içinde bulunan CountVectorizer aracını kullanıyoruz:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d80813",
   "metadata": {},
   "source": [
    "<IMG src=\"bag.jpg\" width=\"900\" height=\"900\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb5e8c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn içinde bulunan countvectorizer fonksiyonunu kullanarak max 5000 kelimelik bag of words oluşturuyoruz...\n",
    "vectorizer = CountVectorizer( max_features = 5000 )\n",
    "\n",
    "# train verilerimizi feature vektöre matrisine çeviriyoruz\n",
    "train_x = vectorizer.fit_transform(train_x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e9925e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<22500x5000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1775691 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a05067bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bunu array'e dönüştürüyoruz çünkü fit işlemi için array istiyor..\n",
    "train_x = train_x.toarray()\n",
    "train_y = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcddaafb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((22500, 5000), (22500,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape, train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c550376c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 1, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d658fe33",
   "metadata": {},
   "source": [
    "### Random Forest Modeli oluşturuyoruz ve fit ediyoruz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0029a57b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(random_state=42)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators = 100, random_state=42)\n",
    "model.fit(train_x, train_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e04f2f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b440b1a5",
   "metadata": {},
   "source": [
    "### Şimdi sıra test datamızda.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5433e2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test verilerimizi feature vektöre matrisine çeviriyoruz\n",
    "# Yani aynı işlemleri(bag of wordse dönüştürme) tekrarlıyoruz bu sefer test datamız için:\n",
    "test_xx = vectorizer.transform(test_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7bbfd5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2500x5000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 199248 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1959911",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_xx = test_xx.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7cf4853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500, 5000)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_xx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef0be3d",
   "metadata": {},
   "source": [
    "#### Prediction yapıyoruz.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54e913a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict = model.predict(test_xx)\n",
    "dogruluk = roc_auc_score(y_test, test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82bbd16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doğruluk oranı : %  83.82325927319812\n"
     ]
    }
   ],
   "source": [
    "print(\"Doğruluk oranı : % \", dogruluk * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dd665d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0733f4e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a750489",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310b3c9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66db775",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3c6d1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31a2173",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31354a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8023d0f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699d4210",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13722a06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3035d62c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6c02ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92cc7c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a362f38d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62913061",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2de634",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becb8202",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37391e33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fe125b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
